#+TITLE: クラスタ分析 
#+SUBTITLE: 非階層的方法と分析の評価
#+AUTHOR: 村田 昇
#+EMAIL: noboru.murata@eb.waseda.ac.jp
#+DATE: 
# Time-stamp: <2021-11-26 15:08:56 mura>
:REVEAL:
#+INCLUDE: "./reveal.js/org/mycourse.org"
#+STARTUP: hidestars content
# C-c C-x C-v でinlineを切り替え
# <m C-i でlatex block (math env用)
# C-c '
:END:

* 講義の内容
# 早稲田大学
- 第1日: クラスタ分析の考え方と階層的方法
- *第2日: 非階層的方法と分析の評価*

#+begin_src R :eval no :exports none :tangle yes
  ### 第11講 資料
#+end_src
#+begin_src R :exports none
  setwd("~/Desktop/lectures/mva/slide")
#+end_src

* COMMENT 講義概要
# 東京大学
- 第1日: クラスタ分析の考え方と階層的方法
- *第2日: 非階層的方法と分析の評価*

#+begin_src R :eval no :exports none :tangle yes
  ### 
  ### 第11講 サンプルコード
  ###
#+end_src
#+begin_src R :exports none
  setwd("~/Desktop/lectures/u-tokyo/autumn/slide")
#+end_src

* COMMENT メモ
- 第3講以降は同じファイルを使うように整理する
- 同じタイトルの項目にはメモを入れる
- 演習の内容が異なるので演習は2つ作る形で対応

* COMMENT 演習 (早稲田大学雛型)
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 早稲田大学
** 問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- について以下を示しなさい．
  - である．
    #+begin_quote
    #+begin_src latex
    #+end_src
    #+end_quote

** 解答例
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- いずれも定義にもとづいて計算すればよい
  #+begin_quote
  #+begin_src latex
  #+end_src
  #+end_quote

* COMMENT 演習 (東京大学雛型)
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 東京大学
** R: 概要
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- ほげほげな方法:
  #+begin_src R :eval no
  #+end_src
  
** データセット
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 以下のデータセットを使用します
  - ~xxx~
    #+begin_quote
    データの素性
    #+end_quote
    [[https://www.foo.com/]]

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 構成しなさい
  - データ
    #+begin_quote
    formulaなどを与える
    #+end_quote

#+begin_src R :eval no :exports none :tangle yes
  ###
  ### 練習問題 
  ###

  ## 
  ## 分析
  ##
#+end_src

** COMMENT 講義資料: 概略
:PROPERTIES:
:reveal_background: #EEEEFF
:END:
- [[./code/bar.r][bar.r]] を確認してみよう

#+begin_src R :eval no :exports none :tangle yes
  ### 第06回 資料
#+end_src
#+begin_src R :exports none
  setwd("~/Desktop/lectures/mva/slide")
#+end_src



* クラスタ分析の復習
** クラスタ分析
- *cluster analysis*
  #+begin_quote
  個体の間に隠れている
  *集まり=クラスタ*
  を個体間の"距離"にもとづいて発見する方法
  #+end_quote
- 個体間の類似度・距離(非類似度)を定義:
  - 同じクラスタに属する個体どうしは似通った性質
  - 異なるクラスタに属する個体どうしは異なる性質
- さらなるデータ解析やデータの可視化に利用
- 教師なし学習の代表的な手法の一つ

** クラスタ分析の考え方
- 階層的方法:
  - データ点およびクラスタの間に *距離* を定義
  - 距離に基づいてグループ化:
    - 近いものから順にクラスタを *凝集*
    - 近いものが同じクラスタに残るように *分割*
- 非階層的方法:
  - クラスタの数を事前に指定
  - クラスタの *集まりの良さ* を評価する損失関数を定義
  - 損失関数を最小化するようにクラスタを形成

** COMMENT 階層的方法における凝集的手続き
1. データ・クラスタ間の距離を定義する
   - データ点とデータ点の距離
   - クラスタとクラスタの距離
2. データ点およびクラスタ間の距離を求める
3. 最も近い2つを統合し新たなクラスタを形成する
4. クラスタ数が1つになるまで2-3の手続きを繰り返す

** 階層的方法における凝集的手続き
#+begin_leftcol60
1. データ・クラスタ間の距離を定義
   - データ点間の距離
   - クラスタ間の距離
2. データ点およびクラスタ間の距離を計算
3. 最も近い2つを統合し新たなクラスタを形成
4. クラスタ数が1つになるまで2-3の手続きを繰り返す
#+end_leftcol60    
#+begin_rightcol40
#+header: :width 600 :height 1000 :res 100
#+begin_src R :file figs/11_hclst.png :exports results :results graphics
  require(cluster)
  JS.data <- read.csv("data/japan_social.csv", row.names=1)
  pref <- read.csv(file="data/prefecture.csv", row.names=1)
  rownames(JS.data) <- pref$jp
  myPlot <- function(k) {
    tmpa <- JS.data[8:14,]
    tmpb <- list(c(1,2,3,4,1,6,7),
		 c(1,2,3,1,1,6,7),
		 c(1,2,2,1,1,6,7),
		 c(1,2,2,1,1,6,1),
		 c(1,1,1,1,1,6,1),
		 c(1,1,1,1,1,1,1))
    clusplot(x=tmpa,
	     clus=c(1,2,3,4,5,6,7),
	     diss=FALSE,
	     stand=TRUE, lines=0, labels=3, 
	     main=NULL, sub=NULL, cex=1,
	     xlab="", ylab="", axes=FALSE,
	     xlim=c(-2.5,1.5), ylim=c(-1.5,2.2),
	     col.p="blue", col.clus="white", shade=FALSE)
    box()
    if(k>0) {
      for(i in 1:k) {
	clusplot(x=tmpa,
		 clus=tmpb[[i]],
		 diss=FALSE,
		 stand=TRUE, add=TRUE, span=FALSE,
		 lines=0, lwd=2, col.p="blue", col.clus="orange")
      }
    }
  }
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  par(mfrow=c(4,2), mar=rep(0.5,4))
  myPlot(0)
  myPlot(1)
  myPlot(2)
  myPlot(3)
  myPlot(4)
  myPlot(5)
  myPlot(6)
  plot(agnes(scale(JS.data[8:14,])), which.plots=2,
       main="",sub="",xlab="")
#+end_src

#+CAPTION: 凝集的手続きの例
#+NAME: fig:11_hclst
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_hclst.png]]
#+end_rightcol40    

   
* 非階層的方法
** 非階層的方法の手続き
- 対象の変数: \(\boldsymbol{X}=(X_1,X_2,\dotsc,X_p)^{\mathsf{T}}\) (\(p\)次元)
- 観測データ: \(n\) 個の個体の組
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      \{\boldsymbol{x}_{i}\}_{i=1}^{n}
      =
      \{(x_{i1},x_{i2},\dotsc,x_{ip})^{\mathsf{T}}\}_{i=1}^{n}
    \end{equation}
  #+end_src
  #+end_quote
- 個体とクラスタの対応 \(C\) を推定:
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      C(i)
      =\text{(個体 \(i\) が属するクラスタ番号)}
    \end{equation}
  #+end_src
  #+end_quote
  - 対応 \(C\) の *全体の良さ* を評価する損失関数を設定
  - 観測データ
    \(\{\boldsymbol{x}_{i}\}_{i=1}^{n}\)
    に最適な対応
    \(\{C(i)\}_{i=1}^{n}\) を決定

** \(k\)-平均法の損失関数
- クラスタの個数 \(k\) を指定
- 2つの個体 \(i,i'\) の *近さ=損失* を距離の二乗で評価
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      \|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2
      =
      \sum_{j=1}^p(x_{ij}-x_{i'j})^2
    \end{equation}
  #+end_src
  #+end_quote
- 損失関数 \(W(C)\): クラスタ内の平均の近さを評価
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      W(C)
      =
      \sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}\|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2
    \end{equation}
  #+end_src
  #+end_quote
  # (\(n_l\) はクラスタ \(l\) に属する個体の総数)

** \(k\)-平均法の性質
- クラスタ \(l\) に属する個体の平均:
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      \bar{\boldsymbol{x}}_l
      =
      \frac{1}{n_l}\sum_{i:C(i)=l}\boldsymbol{x}_i
    \end{equation}
  #+end_src
  #+end_quote
- 損失関数 \(W(C)\) の等価な表現:
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      W(C)
      =
      2\sum_{l=1}^k\sum_{i:C(i)=l}\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2
    \end{equation}
  #+end_src
  #+end_quote
- 最適な対応 \(C\): クラスタ内変動の総和が最小


* 演習
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 早稲田大学
** 問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 以下の問に答えなさい．
  - 損失関数 \(W(C)\) の等価な表現を示しなさい
    #+begin_quote
    #+begin_src latex
      \begin{align}
	W(C)
	&=
   \sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}\|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2\\
	&=
   2\sum_{l=1}^k\sum_{i:C(i)=l}\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2
      \end{align}
    #+end_src
    #+end_quote
  - 以下の\(\hat{\boldsymbol{\mu}}\)を求めなさい
    #+begin_quote
    #+begin_src latex
      \begin{equation}
	\hat{\boldsymbol{\mu}}
	=\arg\min_{\mu}
	\sum_{i:C(i)=l}\|\boldsymbol{x}_i-\boldsymbol{\mu}\|^2
      \end{equation}
    #+end_src
    #+end_quote

** COMMENT 解答例
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 対称性に注意して標本平均のまわりで展開
  #+begin_quote
  #+begin_src latex
    \begin{align}
      &\sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
	\|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2\\
      &=
	\sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
	\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}+\bar{\boldsymbol{x}}_{l}-\boldsymbol{x}_{i'}\|^2\\
      &=
	\sum_{l=1}^k\frac{2}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
	\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2\\
      &\quad-
	\sum_{l=1}^k\frac{2}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
	(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l})^{\mathsf{T}}
	(\boldsymbol{x}_{i'}-\bar{\boldsymbol{x}}_{l})
    \end{align}
  #+end_src
  #+end_quote

#+reveal: split
- 中心化したデータの標本平均が0であることを利用
  #+begin_quote
  #+begin_src latex
    \begin{align}
      &=
	2\sum_{l=1}^k\sum_{i:C(i)=l}
	\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2\\
      &\quad-
	\sum_{l=1}^k\frac{2}{n_l}
	\sum_{i:C(i)=l}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l})^{\mathsf{T}}
	\sum_{i':C(i')=l}(\boldsymbol{x}_{i'}-\bar{\boldsymbol{x}}_{l})\\
      &=
	2\sum_{l=1}^k\sum_{i:C(i)=l}
	\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2
    \end{align}
  #+end_src
  #+end_quote

#+reveal: split
- 以下の不等式が成立
  #+begin_quote
  #+begin_src latex
    \begin{align}
      \sum_{i:C(i)=l}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}\|^{2}
      &=
	\sum_{i:C(i)=l}
	\|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}
	+
	\bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu}\|^{2}\\
      &=
	\sum_{i:C(i)=l}
	\|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}\|^{2}
	+
	\sum_{i:C(i)=l}
	\|\bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu}\|^{2}\\
      &\quad+
	2\sum_{i:C(i)=l}
	(\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l})^{\mathsf{T}}
	(\bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu})\\
      &=
	\sum_{i:C(i)=l}
	\|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}\|^{2}
	+
	n_{l}\|\bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu}\|^{2}\\
      &\ge
	\sum_{i:C(i)=l}
	\|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}\|^{2}
    \end{align}
  #+end_src
  #+end_quote

#+reveal: split
- 等号の成立の条件より
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      \hat{\boldsymbol{\mu}}
      =\arg\min_{\mu}
      \sum_{i:C(i)=l}\|\boldsymbol{x}_i-\boldsymbol{\mu}\|^2
      =\bar{\boldsymbol{x}}_{l}
    \end{equation}
  #+end_src
  クラスタの標本平均を中心とすればよい
  #+end_quote


* 近似的な最適化
** クラスタ対応の最適化
- 最適化: 損失関数 \(W(C)\) を最小とする \(C\) を決定
- 貪欲な \(C\) の探索:
  - 原理的には全ての値を計算すればよい
  - 可能な \(C\) の数: \(k^n\) 通り (有限個のパターン)
  - サンプル数 \(n\) が小さくない限り実時間での実行は不可能
- 近似的な \(C\) の探索:
  - いくつかのアルゴリズムが提案されている
  - 基本的な考え方: *Lloyd-Forgyのアルゴリズム*
    #+begin_quote
    標本平均と変動の平方和の性質を利用
    #+begin_src latex
      \begin{equation}
	\bar{\boldsymbol{x}}_l
	=\arg\min_{\mu}
	\sum_{i:C(i)=l}\|\boldsymbol{x}_i-\boldsymbol{\mu}\|^2
	\quad
	\text{(クラスタ\(l\)の標本平均)}
      \end{equation}
    #+end_src
    #+end_quote

** Lloyd-Forgyのアルゴリズム
1. クラスタ中心の初期値 
   \(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_k\) を与える
2. 各データの所属クラスタ番号 \(C(i)\) を求める
   #+begin_quote
   #+begin_src latex
     \begin{equation}
       C(i)
       =
       \arg\min_l\|\boldsymbol{x}_i-\boldsymbol{\mu}_l\|
     \end{equation}
   #+end_src
   #+end_quote
3. 各クラスタ中心 \(\boldsymbol{\mu}_l\;(l=1,2,\dotsc,k)\) を更新する
   #+begin_quote
   #+begin_src latex
     \begin{equation}
       \boldsymbol{\mu}_l
       =
       \frac{1}{n_l}\sum_{i:C(i)=l}\boldsymbol{x}_i,
       \quad
       n_l=|\{\boldsymbol{x}_i|C(i)=l\}|
     \end{equation}
   #+end_src
   # (\(n_l\) は \(C(i)=l\) となるデータの総数)
   #+end_quote
4. 中心が変化しなくなるまで 2,3 を繰り返す

** アルゴリズムの性質
- 結果は確率的
  - 初期値 \(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_k\) に依存
  - アルゴリズムの成否は確率的 \\
    (最適解が得られない場合もある)
- 一般には複数の初期値をランダムに試して損失を最小とする解を採用する
- 平均の代わりにメドイド (medoid; 中心にある観測値) を用いる方法もある
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      \boldsymbol{x}^{medoid}_{l}
      =\arg\min_{\boldsymbol{x}_{i}}
      \sum_{i':C(i')=l}
      \|\boldsymbol{x}_{i}-\boldsymbol{x}_{i'}\|^2
    \end{equation}
  #+end_src
  #+end_quote

** 事例
- 都道府県別好きなおむすびの具(一部)での例
  #+header: :width 720 :height 720 :res 100
  #+begin_src R :file figs/11_nhclst0.png :exports results :results graphics
    OM.data <- read.csv(file="data/omusubi.csv", row.names=1)
    pref <- read.csv(file="data/prefecture.csv", row.names=1)
    rownames(OM.data) <- pref$jp

    OM.subset <- OM.data[-c(8:14,24:30),]

    n <- nrow(OM.subset)

    jdx <- 1:ncol(OM.subset) # jdx <- sample(ncol(OM.subset),2)
    OM.pca <- prcomp(OM.subset[,jdx])

    if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
    plot(predict(OM.pca),
	 col="blue", pch=19, cex=0.5,
	 xlab="Component 1", ylab="Component 2")
    text(transform(predict(OM.pca), PC2=PC2+0.5),
	 labels=rownames(OM.subset), col="blue", cex=1)
  #+end_src

#+CAPTION: 非階層的クラスタリング
#+NAME: fig:11_nhclst0
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst0.png]]

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_nhclst1.png :exports results :results graphics
  ## 図示のための関数
  myPlot <- function(cntr) {
    dsy <- daisy(rbind(OM.subset[,jdx],cntr))
    clst <- apply(as.matrix(dsy)[1:n,(1:k)+n],
		  1,
		  function(x){which.min(x)})
    plot(predict(OM.pca), col=clst, pch=clst, type="n",
	 xlab="Component 1", ylab="Component 2")
    text(predict(OM.pca), labels=clst, col=clst, cex=1.5)
    points(predict(OM.pca,newdata=cntr), col=1:k, pch=19, cex=2)
    cntr <- aggregate(. ~ clst,
		      data=data.frame(OM.subset[,jdx],clst),
		      mean)[,-1]
    return(cntr)
  }
  set.seed(1212)
  k <- 5
  idx <- sample(n,k)
  cntr <- OM.subset[idx,jdx]
  ## 
  cntr <- myPlot(cntr)
#+end_src

#+CAPTION: Lloyd-Forgyのアルゴリズム (その1)
#+NAME: fig:11_nhclst1
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst1.png]]

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_nhclst2.png :exports results :results graphics
  cntr <- myPlot(cntr)
#+end_src

#+CAPTION: Lloyd-Forgyのアルゴリズム (その2)
#+NAME: fig:11_nhclst2
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst2.png]]

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_nhclst3.png :exports results :results graphics
  cntr <- myPlot(cntr)
#+end_src
 
#+CAPTION: Lloyd-Forgyのアルゴリズム (その3)
#+NAME: fig:11_nhclst3
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst3.png]]

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_nhclst4.png :exports results :results graphics
   cntr <- myPlot(cntr)
#+end_src
#+CAPTION: Lloyd-Forgyのアルゴリズム (その4)
#+NAME: fig:11_nhclst4
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst4.png]]

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_nhclst5.png :exports results :results graphics
   cntr <- myPlot(cntr)
#+end_src
#+CAPTION: Lloyd-Forgyのアルゴリズム (その5)
#+NAME: fig:11_nhclst5
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst5.png]]

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_nhclst6.png :exports results :results graphics
   cntr <- myPlot(cntr)
#+end_src
#+CAPTION: Lloyd-Forgyのアルゴリズム (その6)
#+NAME: fig:11_nhclst6
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst6.png]]

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_nhclst.png :exports results :results graphics
   if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
   dsy <- daisy(rbind(OM.subset[,jdx],cntr))
   clst <- apply(as.matrix(dsy)[1:n,(1:k)+n],
                 1,
                 function(x){which.min(x)})
   plot(predict(OM.pca),
        col="blue", pch=19, cex=0.5,
        xlab="Component 1", ylab="Component 2")
   text(transform(predict(OM.pca), PC2=PC2+0.5),
        labels=rownames(OM.subset), col=clst, cex=1)
#+end_src
#+CAPTION: クラスタリングの結果
#+NAME: fig:11_nhclst
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_nhclst.png]]
 

* COMMENT 演習
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 東京大学
** R: 関数 ~kmeans()~
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- \(k\)-平均法を実行するための標準的な関数
  #+begin_src R :eval no
    kmeans(x, centers, iter.max = 10, nstart = 1,
	   algorithm = "Hartigan-Wong")
    ## x: データフレーム
    ## centers: クラスタ数
    ## iter.max: 最大繰り返し数
    ## nstart: 初期値の候補数
    ## algorithm: 最適化法の指定．他に "Lloyd", "Forgy", "MacQueen" が指定可
  #+end_src
- 結果は変数のスケールにも依存
  - 例えば測定値の単位により異なる
  - 必要ならば主成分分析の場合と同様にデータの標準化を行う

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- データセット ~japan_social.csv~ を用いて
  以下を確認しなさい
  #+begin_src R :eval no
    JS.data <- read.csv("data/japan_social.csv", row.names=1)
  #+end_src
  - 関数 ~kmeans()~ を用いて
    各変数平均0，分散1に標準化
    (関数 ~scale()~ を利用)
    したデータを7クラスタに分割しなさい
  - 各クラスタ内の県名を表示しなさい
  - 関数 ~clusplot()~ を用いて
    2次元散布図に各クラスタを表示しなさい

#+begin_src R :eval no :exports none :tangle yes
  ### 練習1.1
  ### 関数 kmeans による非階層的クラスタリング
  
  ## データの読み込み
  JS.data <- read.csv(file="data/japan_social.csv", row.names=1)
  
  ## k-平均法の実行: 
  set.seed(1234) # 必要に応じて初期値の乱数のシード
  k <- 7 # 分割数を指定
  JS.km <- kmeans(scale(JS.data), # 標準化
		  centers=k, # クラスタ数
		  nstart=20) # 初期値を20回変更して試す
  
  ## 各クラスター内の県名を表示
  for(i in 1:k){
    cat("<< cluster",i,">>\n")
    print(names(which(JS.km$cluster==i)))
  }
  
  ## clusplotによるクラスタの表示
  library(cluster) # clusplot を利用するため
  clusplot(x=JS.data,
	   clus=JS.km$cluster, # クラスタ番号
	   stand=TRUE, # 標準化したデータで表示
	   lines=0, labels=3, # 表示の指定
	   main=NULL, sub=NULL, cex=0.8, # タイトルなどの調整
	   col.p=rainbow(k)[JS.km$cluster], # 虹色で色付け
	   col.clus="orange", shade=FALSE)	 # クラスタ囲みの指定
#+end_src

** R: 関数 ~cluster::pam()~
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- \(k\)-メドイド法を実行するための関数
  #+begin_src R :eval no
    pam(x, k, metric = "euclidean", stand = FALSE)
    ## x: データフレーム，または距離行列 
    ## k: クラスタの数
    ## metric: 距離の指定(xがデータフレームの場合)．他に "manhattan" が指定可
    ## stand: 標準化(平均0，絶対偏差1)
  #+end_src
  - 詳細は ~?cluster::pam~ を参照

** COMMENT 演習: 非階層的クラスタリング
:PROPERTIES:
:reveal_background: #EEEEFF
:END:
- [[./code/12-kmeans.r][12-kmeans.r]] を確認してみよう

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- データセット ~japan_social.csv~ を用いて
  以下を確認しなさい
  - 関数 ~pam()~ を用いて
    各変数平均0，絶対偏差1に標準化したデータを7クラスタに分割しなさい
  - 各クラスタ内の県名を表示しなさい
  - 関数 ~clusplot()~ を用いて
    2次元散布図に各クラスタを表示しなさい
- 余裕がある人はデータセット ~omusubi.csv~ でも確認しなさい

#+begin_src R :eval no :exports none :tangle yes
  ### 練習1.2
  ### 関数 pam による非階層的クラスタリング

  ## k-medoids の実行
  JS.pam <- pam(JS.data,
                stand=TRUE,
                k=k)

  ## 各クラスター内の県名を表示
  for(i in 1:k){
      cat("<< cluster",i,">>\n")
      print(names(which(JS.pam$clustering==i)))
  }

  ## クラスタの表示
  clusplot(x=JS.data,
           clus=JS.pam$clustering,
           stand=TRUE,
           lines=0, labels=3, 
           main=NULL, sub=NULL, cex=0.8,
           col.p=rainbow(k)[JS.pam$clustering],
           col.clus="orange", shade=FALSE)

  ## 
  OM.data <- read.csv(file="data/omusubi.csv", row.names=1)

  ## k-medoids の実行
  k <- 6
  OM.pam <- pam(daisy(sqrt(OM.data)), # Hellinger距離 (スケールをさぼっている)
                k=k)

  ## 各クラスター内の県名を表示
  for(i in 1:k){
      cat("<< cluster",i,">>\n")
      print(names(which(OM.pam$clustering==i)))
  }

  ## クラスタの表示
  clusplot(x=OM.data,
           clus=OM.pam$clustering,
           stand=TRUE,
           lines=0, labels=3, 
           main=NULL, sub=NULL, cex=0.8,
           col.p=rainbow(k)[OM.pam$clustering],
           col.clus="orange", shade=FALSE)
#+end_src


* 解析事例
** 都道府県別の社会生活統計指標
- データの属性
  #+begin_quote
  - Forest: 森林面積割合 (%) 2014年
  - Agri: 就業者１人当たり農業産出額(販売農家）(万円) 2014年
  - Ratio: 全国総人口に占める人口割合 (%) 2015年
  - Land: 土地生産性（耕地面積１ヘクタール当たり）(万円) 2014年
  - Goods: 商業年間商品販売額［卸売業＋小売業］（事業所当たり）(百万円) 2013年
  #+end_quote
- 平均0，分散1に正規化して解析

#+reveal: split
- ユークリッド距離 + k-平均法 
  #+begin_src R :exports results :tangle yes
    ### 総務省統計局の統計データによる例
    require(cluster)
    require(RColorBrewer)
    JS.data <- read.csv(file="data/japan_social.csv", row.names=1)
    pref <- read.csv(file="data/prefecture.csv", row.names=1)
    rownames(JS.data) <- pref$jp

    ## k-平均法の実行: 
    set.seed(1234)
    k <- 7 # 分割数を指定
    JS.km <- kmeans(scale(JS.data), centers=k, nstart=20)

    ## 結果の確認 (各クラスター内の県名を表示)
    for(i in 1:k){
	   cat("<< cluster",i,">>\n")
	   print(pref$jp[JS.km$cluster==i])
    }
  #+end_src
  
#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_jskmeans.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  clusplot(x=JS.data,
	      clus=JS.km$cluster,
	      diss=FALSE,
	      stand=TRUE, lines=0, labels=3, 
	      main=NULL, sub=NULL, cex=0.8,
	      col.p=brewer.pal(k,"Dark2")[JS.km$cluster],
	      col.clus="orange", shade=FALSE)
#+end_src
#+CAPTION: ユークリッド距離 + k-平均法 
#+NAME: fig:11_jskmeans
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_jskmeans.png]]

#+reveal: split
- ユークリッド距離 + k-メドイド法 
  #+begin_src R :exports results :tangle yes
    ## k-medoids の実行
    JS.pam <- pam(scale(JS.data),
		     k=k)
		     ## metric="manhattan", stand=TRUE)

    ## 結果の確認 (各クラスター内の県名を表示)
    for(i in 1:k){
	 cat("<< cluster",i,">>\n")
	 print(pref$jp[JS.pam$clustering==i])
    }
  #+end_src

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_jspam.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  clusplot(x=JS.data,
	      clus=JS.pam$clustering,
	      diss=FALSE,
	      stand=TRUE, lines=0, labels=3, 
	      main=NULL, sub=NULL, cex=0.8,
	      col.p=brewer.pal(k,"Dark2")[JS.pam$clustering],
	      col.clus="orange", shade=FALSE)
#+end_src
#+CAPTION: ユークリッド距離 + k-メドイド法 
#+NAME: fig:11_jspam
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_jspam.png]]

** 都道府県別好きなおむすびの具
- データの属性
  #+begin_quote
  : Q2. おむすびの具では何が一番好きですか？
  :	   A.梅 B.鮭 C.昆布 D.かつお E.明太子 F.たらこ Ｇ.ツナ H.その他
  : 【回答者数】
  : 男性	9,702人	    32.0%
  : 女性    20,616人	    68.0%
  : 総数    30,318人	   100.0%
  に対する回答を県別に集計
  #+end_quote
- Hellinger距離を利用
  #+begin_quote
  \(\boldsymbol{p},\boldsymbol{q}\)
  を確率ベクトルとして
  定義される確率分布の距離
  #+begin_src latex
    \begin{equation}
      d_{hel}(\boldsymbol{p},\boldsymbol{q})
      =
      \frac{1}{\sqrt{2}}d_{euc}(\sqrt{\boldsymbol{p}},\sqrt{\boldsymbol{q}})
    \end{equation}
  #+end_src
  #+end_quote

#+reveal: split
- Hellinger距離 + k-メドイド法 
  #+begin_src R :exports results :tangle yes
    OM.data <- read.csv(file="data/omusubi.csv", row.names=1)
    pref <- read.csv(file="data/prefecture.csv", row.names=1)
    rownames(OM.data) <- pref$jp

    ## k-medoids の実行
    k <- 6
    OM.pam <- pam(daisy(sqrt(OM.data)),
		  k=k,
		  stand=TRUE)

    ## 結果の確認 (各クラスター内の県名を表示)
    for(i in 1:k){
        cat("<< cluster",i,">>\n")
        print(pref$jp[OM.pam$clustering==i])
    }
  #+end_src

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_ompam.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  clusplot(x=OM.data,
	   clus=OM.pam$clustering,
	   diss=FALSE,
	   stand=TRUE, lines=0, labels=3, 
	   main=NULL, sub=NULL, cex=0.8,
	   col.p=brewer.pal(k,"Dark2")[OM.pam$clustering],
	   col.clus="orange", shade=FALSE)
#+end_src

#+CAPTION: Hellinger距離 + k-メドイド法 
#+NAME: fig:11_ompam
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_ompam.png]]
   
   
* クラスタ構造の評価
** 階層的方法の評価
- 評価の対象
  - データ \(\boldsymbol{x}_i\) と最初に統合されたクラスタ \(C\) の距離:
    #+begin_quote
    #+begin_src latex
      \begin{equation}
	d_i
	=
	D({\boldsymbol{x}_i},C)
      \end{equation}
    #+end_src
    #+end_quote
  - 最後に統合された2つのクラスタ \(C',C''\) の距離:
    #+begin_quote
    #+begin_src latex
      \begin{equation}
	D
	=
	D(C',C'')
      \end{equation}
    #+end_src
    #+end_quote
- *凝集係数* (agglomerative coefficient):
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      AC
      =
      \frac{1}{n}\sum_{i=1}^{n}\left(1-\frac{d_i}{D}\right)
    \end{equation}
  #+end_src
  #+end_quote

** 凝集係数の性質
- 定義より
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      0\le AC\le 1
    \end{equation}
  #+end_src
  #+end_quote
- 1に近いほどクラスタ構造が明瞭
- banner plot の面積比として視覚化 \\ 
  (banner plot: \((1-{d_i}/{D})\) をデータ毎に並べた棒グラフ)

** COMMENT 非階層的方法の評価
- 評価の対象
  - \(\boldsymbol{x}_i\) を含むクラスタ \(C^1\) と \(\boldsymbol{x}_i\) の距離:
    #+begin_quote
    #+begin_src latex
      \begin{equation}
	d^1_i=D({\boldsymbol{x}_i},C^1\setminus{\boldsymbol{x}_i})
      \end{equation}
    #+end_src
    #+end_quote
  - 一番近いクラスタ \(C^2\) と \(\boldsymbol{x}_i\) の距離:
    #+begin_quote
    #+begin_src latex
      \begin{equation}
	d^2_i=D({\boldsymbol{x}_i},C^2)
      \end{equation}
    #+end_src
    #+end_quote
- *シルエット係数* (silhouette coefficient):
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      S_i
      =
      \frac{d^2_i-d^1_i}{\max(d^1_i,d^2_i)}
    \end{equation}
  #+end_src
  #+end_quote

  # #   - データ \(\boldsymbol{x}_i\) が含まれているクラスタ: \(C^1\)
  # #   - \(C^1\) 以外で \(\boldsymbol{x}_i\) に一番近いクラスタ: \(C^2\)
  # # - \(\boldsymbol{x}_i\) を除いたクラスタ \(C^1\) とデータ \(\boldsymbol{x}_i\) の距離:
  # #   # #+begin_export latex
  # #   \begin{equation}
  # #	 d^1_i
  # #	 =
  # #	 D({\boldsymbol{x}_i},C^1\setminus{\boldsymbol{x}_i})
  # #   \end{equation}
  # #   # #+end_export
  # # - \(C^2\) と \(\boldsymbol{x}_i\) の距離:
  # #   # #+begin_export latex
  # #   \begin{equation}
  # #	 d^2_i
  # #	 =
  # #	 D({\boldsymbol{x}_i},C^2)
  # #   \end{equation}
  # #   # #+end_export

** シルエット係数の性質
- 定義より
  #+begin_quote
  #+begin_src latex
    \begin{equation}
      -1\le S_i\le 1
    \end{equation}
  #+end_src
  #+end_quote
- 1に近いほど適切なクラスタリング
- 全体の良さを評価するには \(S_i\) の平均を用いる
- 距離の計算を適切に行えば階層的方法でも利用可


* 演習
** 問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 以下の問に答えなさい．
  - 群平均法において凝集係数が以下を満たすことを示しなさい
    #+begin_quote
    #+begin_src latex
      \begin{equation}
	0\le AC\le 1
      \end{equation}
    #+end_src
    #+end_quote
  - シルエット係数が以下を満たすことを示しなさい
    #+begin_quote
    #+begin_src latex
      \begin{equation}
	-1\le S_i\le 1
      \end{equation}
    #+end_src
    #+end_quote

** COMMENT 解答例
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 2つのクラスタ\(C_{a},C_{b}\)が最も近いとする
  #+begin_quote
  #+begin_src latex
    \begin{equation}
	 D(C_{c},C_{d})\ge D(C_{a},C_{b}),
	 \quad\forall c,d
    \end{equation}
  #+end_src
  #+end_quote

#+reveal: split
- 統合して計算される距離では下が成立
  #+begin_quote
  #+begin_src latex
    \begin{align}
	 D(C_{a}+C_{b}, C_{c})
	 &=
	   \frac{|C_{a}|D(C_{a},C_{c})+|C_{b}|D(C_{b},C_{c})}{|C_{a}|+|C_{b}|}\\
	 &\ge
	   \frac{|C_{a}|D(C_{a},C_{b})+|C_{b}|D(C_{a},C_{b})}{|C_{a}|+|C_{b}|}\\
	 &=D(C_{a},C_{b})
    \end{align}
  #+end_src
  統合した結果，それより短い距離が現れることはない
  #+end_quote

#+reveal: split
- 以上より
  #+begin_quote
  #+begin_src latex
    \begin{equation}
	 0\le d_{i}\le D
    \end{equation}
    \begin{equation}
	 0\le 1-\frac{d_i}{D}\le 1
    \end{equation}
  #+end_src
  よって
  #+begin_src latex
    \begin{equation}
	 0\le AC\le 1
    \end{equation}
  #+end_src
  #+end_quote

#+reveal: split
- 非負値の大小関係に注意する
  #+begin_quote
  #+begin_src latex
    \begin{equation}
	 -\max(d^{1}_{i},d^{2}_{i})\le d^{2}_{i}-d^{1}_{i}\le\max(d^{1}_{i},d^{2}_{i})
    \end{equation}
  #+end_src
  より
  #+begin_src latex
    \begin{equation}
	 -1\le S_i\le 1
    \end{equation}
  #+end_src
  #+end_quote
  

* COMMENT 演習
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 東京大学
** R: 関数 ~cluster::agnes()~
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 関数 ~agnes()~ による凝集係数の取得
  #+begin_src R :eval no
    ### 凝集係数の取得
    summary(agnes(x, # データフレーム
		  metric="euclidean", stand=TRUE, # 正規化・ユークリッド距離
		  method="average") # 群平均法
	    )$ac 
    ### 凝集係数の視覚化 (banner plot)
    plot(agnes(x), which.plots=1)
    ## plot および cluster::bannerplot のオプションを参考
  #+end_src

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- データセット ~japan_social.csv~ を用いて
  以下を検討しなさい．
  - 関数 ~agnes()~ を用いて階層的クラスタリングを行いなさい
    - 標準化: 行う
    - データ距離: ユークリッド距離，およびマンハッタン距離
    - クラスタ距離: 群平均法
  - 凝集係数を用いて2つの距離の評価を行いなさい

#+begin_src R :eval no :exports none :tangle yes
  ### 練習2.1
  ### 凝集係数による距離の検討

  ## データの読み込み (既に読み込んでいれば不要)
  JS.data <- read.csv("data/japan_social.csv", row.names=1)

  ## ユークリッド距離による階層的クラスタリング
  JS.agn.euc <- agnes(JS.data,
			metric="euclidean", # データ距離
			stand=TRUE,	    # 標準化
			method="average")   # クラスタ距離
  plot(JS.agn.euc, which.plots=2, # デンドログラムの表示
	    main="euclidean") 

  ## マンハッタン距離による階層的クラスタリング
  JS.agn.man <- agnes(JS.data,
			metric="manhattan",
			stand=TRUE,
			method="average")
  plot(JS.agn.man, which.plots=2,
	    main="manhattan")

  ## データ毎の凝集係数の表示
  plot(JS.agn.euc, which.plots=1, # banner plotの表示
	    nmax.lab=50,   # 表示するラベルの上限 (標準は40)
	    max.strlen=5,  # 表示するラベルの文字数の上限
	    main="euclidean")

  plot(JS.agn.man, which.plots=1,
	    nmax.lab=50,  
	    max.strlen=5,
	    main="manhattan")

  ## 一部のデータの距離が大きいと凝集係数は大きくなりがち (理由を考えてみよう)
  ## 北海道，東京，宮崎，鹿児島を除いて再計算する 
  summary(agnes(JS.data[-c(1,13,45,46),],
		     metric="euclidean",
		     stand=TRUE,
		     method="average"))$ac
  summary(agnes(JS.data[-c(1,13,45,46),],
		     metric="manhattan",
		     stand=TRUE,
		     method="average"))$ac
  ## ユークリッド距離の方が凝集係数は大きいことがわかる
  ## 個別の係数の確認
  plot(agnes(JS.data[-c(1,13,45,46),],
		  metric="euclidean",
		  stand=TRUE,
		  method="average"),
	    which.plots=1,
	    nmax.lab=50,  
	    max.strlen=5,
	    main="euclidean")
  plot(agnes(JS.data[-c(1,13,45,46),],
		  metric="manhattan",
		  stand=TRUE,
		  method="average"),
	    which.plots=1,
	    nmax.lab=50,  
	    max.strlen=5,
	    main="manhattan")
#+end_src

** R: 関数 ~cluster::pam()~
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 関数 ~pam()~ によるシルエット係数の取得
  #+begin_src R :eval no
    ### シルエット係数関連の情報取得
    summary(pam(x, k))$silinfo
    ### 各データのシルエット係数
    summary(pam(x, k))$silinfo$widths
    ### 各クラスタのシルエット係数の平均
    summary(pam(x, k))$silinfo$clus.avg.widths
    ### シルエット係数の平均
    summary(pam(OM.dsy,k=k))$silinfo$avg.width
    ### シルエット係数の視覚化 (silhouette plot)
    plot(pam(OM.dsy,k=k), which.plot=2)
    ## plot および cluster::silhouette のオプションを参考
  #+end_src

** COMMENT 演習: クラスタ分析の評価
:PROPERTIES:
:reveal_background: #EEEEFF
:END:
- [[./code/12-eval.r][12-eval.r]] を確認してみよう

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- データセット ~omusubi.csv~ を用いて
  以下を検討しなさい．
  - Hellinger距離を用いて距離行列を計算しなさい
    #+begin_quote
    \(\boldsymbol{p},\boldsymbol{q}\)
    を確率ベクトルとして
    定義される確率分布の距離
    #+begin_src latex
      \begin{equation}
	d_{hel}(\boldsymbol{p},\boldsymbol{q})
	=
	\frac{1}{\sqrt{2}}d_{euc}(\sqrt{\boldsymbol{p}},\sqrt{\boldsymbol{q}})
      \end{equation}
    #+end_src
    #+end_quote
  - クラスタ数4-10のシルエット係数を比較しなさい
  - 適当と思われるクラスタ数による分析を行いなささい

#+begin_src R :eval no :exports none :tangle yes
  ### 練習2.2
  ### シルエット係数によるクラスタ数の検討

  ## データの読み込み (既に読み込んでいれば不要)
  OM.data <- read.csv(file="data/omusubi.csv", row.names=1)

  ## Hellinger距離の計算
  OM.dsy <- 1/sqrt(2)*daisy(sqrt(OM.data/100))

  ## クラスタ数 4-10 で平均シルエット係数を確認
  for(k in 4:10){
	   cat("k =",k," ")
	   print(summary(pam(OM.dsy,k=k))$silinfo$avg.width)
  }

  ## 7-9 のシルエット係数を視覚化
  plot(pam(OM.dsy,k=7), which.plot=2,
	    nmax.lab=50, # 表示するラベルの上限 (標準は40)
	    max.strlen=5, # 表示するラベルの文字数の上限
	    cex.names=0.1) # ラベルの文字の大きさの調整
  plot(pam(OM.dsy,k=8), which.plot=2,
	    nmax.lab=50, max.strlen=5, cex.names=0.1)
  plot(pam(OM.dsy,k=9), which.plot=2,
	    nmax.lab=50, max.strlen=5, cex.names=0.1)

  ## k=8が悪いシルエット係数が少ないという意味で良さそうなのでクラスタの結果を表示
  k <- 8
  OM.pam <- pam(OM.dsy,k=k)
  plot(OM.pam, which.plot=1, 
	    stand=TRUE,
	    lines=0, labels=3, 
	    main="", sub=NULL, cex=0.8, # タイトルと文字の大きさの調整
	    col.p=rainbow(k)[OM.pam$clustering], # クラスタ番号ごとに色付け
	    col.clus="orange", shade=FALSE) # クラスタを楕円で表示
  ## cluster::clusplot のオプションを参考
  ## クラスタリングの結果は OM.pam$clustering に保管されている
#+end_src


* 解析事例
** 都道府県別の社会生活統計指標
- 凝集係数を用いて階層的方法の距離を検討
- ユークリッド距離とマンハッタン距離を比較
  - 正規化は共通 (平均0，絶対偏差1)
  - クラスタ距離は群平均法
    
#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_jsbannereuc.png :exports results :results graphics :tangle yes
  JS.agn.euc <- agnes(JS.data,
		      metric="euclidean",
		      stand=TRUE,
		      method="average")
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  plot(JS.agn.euc, which.plots=1,
	  nmax.lab=50, 
	  max.strlen=5, 
	  cex.axis=0.5,
	  main="")
#+end_src

#+CAPTION: 凝集係数 (ユークリッド距離)
#+NAME: fig:11_jsbannereuc
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
file:figs/11_jsbannereuc.png
  
#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_jsdendroeuc.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  plot(JS.agn.euc, which.plots=2,
	  cex=0.8, xlab="", main="")
#+end_src

#+CAPTION: デンドログラム (ユークリッド距離)
#+NAME: fig:11_jsdendroeuc
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
file:figs/11_jsdendroeuc.png

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_jsbannerman.png :exports results :results graphics :tangle yes
  JS.agn.man <- agnes(JS.data,
		      metric="manhattan",
		      stand=TRUE,
		      method="average")
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  plot(JS.agn.man, which.plots=1,
	  nmax.lab=50, 
	  max.strlen=5, 
	  cex.axis=0.5,
	  main="")
#+end_src

#+CAPTION: 凝集係数 (マンハッタン距離)
#+NAME: fig:11_jsbannerman
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
file:figs/11_jsbannerman.png
  
#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_jsdendroman.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  plot(JS.agn.man, which.plots=2,
	  cex=0.8, xlab="", main="")
#+end_src

#+CAPTION: デンドログラム (マンハッタン距離)
#+NAME: fig:11_jsdendroman
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
file:figs/11_jsdendroman.png

#+reveal: split
- 一部のデータの距離が大きいと凝集係数は大きくなりがち (理由を考えてみよう)
- 北海道，東京，宮崎，鹿児島を除いて再計算する 
  #+begin_src R :exports results :tangle yes
    cat("凝集係数 (ユークリッド距離)\n")
    summary(agnes(JS.data[-c(1,13,45,46),],
		     metric="euclidean",
		     stand=TRUE,
		     method="average"))$ac
    cat("凝集係数 (マンハッタン距離)\n")
    summary(agnes(JS.data[-c(1,13,45,46),],
		     metric="manhattan",
		     stand=TRUE,
		     method="average"))$ac
  #+end_src
     
** 都道府県別好きなおむすびの具
- シルエット係数を用いて非階層的方法のクラスタ数を検討
  - データ距離はHellinger距離
  - クラスタ距離は群平均法
- クラスタ数を4-10として比較
  #+begin_src R :exports results :tangle yes
    ## さまざまなクラスタ数で平均シルエット係数を確認
    OM.dsy <- daisy(sqrt(OM.data))
    cat("シルエット係数\n")
    for(k in 4:10){
	   cat("k =",k," ")
	   print(summary(pam(OM.dsy,k=k))$silinfo$avg.width)
    }
  #+end_src

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_omsil7.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  plot(pam(OM.dsy,k=7), which.plot=2,
	  nmax.lab=50, 
	  max.strlen=5, 
	  cex.names=0.5,
	  main="")
#+end_src

#+CAPTION: シルエット係数 (k=7)
#+NAME: fig:11_omsil7
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
file:figs/11_omsil7.png

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_omsil8.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  plot(pam(OM.dsy,k=8), which.plot=2,
	  nmax.lab=50, 
	  max.strlen=5, 
	  cex.names=0.5,
	  main="")
#+end_src

#+CAPTION: シルエット係数 (k=8)
#+NAME: fig:11_omsil8
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
file:figs/11_omsil8.png

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_omsil9.png :exports results :results graphics :tangle yes
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  plot(pam(OM.dsy,k=9), which.plot=2,
	  nmax.lab=50, 
	  max.strlen=5, 
	  cex.names=0.5,
	  main="")
#+end_src

#+CAPTION: シルエット係数 (k=9)
#+NAME: fig:11_omsil9
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
file:figs/11_omsil9.png

#+reveal: split
#+header: :width 720 :height 720 :res 100
#+begin_src R :file figs/11_omclusplot8.png :exports results :results graphics :tangle yes
  ## k=8が良さそう
  if(Sys.info()["sysname"]=="Darwin"){par(family="HiraginoSans-W4")}
  k <- 8
  OM.pam <- pam(OM.dsy,k=k)
  plot(OM.pam, which.plot=1,
	  stand=TRUE, lines=0, labels=3,
	  main="", vsub=NULL, cex=0.8,
	  col.p=brewer.pal(k,"Dark2")[OM.pam$vclustering],
	  col.clus="orange", shade=FALSE)
#+end_src

#+CAPTION: 非階層的クラスタリング (k=8)
#+NAME: fig:11_omclusplot8
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_omclusplot8.png]]


* 次週の予定
  - *第1日: 時系列の基本モデル*
  - 第2日: モデルの推定と予測  

* COMMENT ローカル変数
# Local Variables:
# org-latex-listings: minted
# End:
