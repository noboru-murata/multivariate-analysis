#+TITLE: クラスタ分析 
#+SUBTITLE: 非階層的方法と分析の評価
#+AUTHOR: 村田 昇
#+EMAIL: noboru.murata@gmail.com
#+DATE: 
#+STARTUP: hidestars content indent
# Time-stamp: <2023-12-18 16:13:00 mura>
:REVEAL:
#+SETUPFILE: "./reveal.js/org/mycourse.org"
# C-c C-x C-v でinlineを切り替え
# <m C-i でlatex block (math env用)
# C-c '
:END:

* COMMENT メモ
[[file:README.org::第11講]]
  
* 講義の内容
:PROPERTIES:
:ID:       1957D10C-281F-41A0-ACE9-1119E2E55796
:END:
# 早稲田大学
- 第1回 : クラスタ分析の考え方と階層的方法
- *第2回 : 非階層的方法と分析の評価*

#+begin_src R :exports none :tangle no
  setwd("~/Desktop/lectures/mva/course")
#+end_src
#+begin_src R :exports none
  ### 第11講 資料
  library(conflicted)
  conflicts_prefer(
    dplyr::filter(),
    dplyr::select(),
    dplyr::lag(),
    )
  library(tidyverse)
  library(GGally)
  library(ggfortify)
  library(cluster)
  library(ggdendro)
  #' 日本語表示・色の設定 (ggplot)
  theme_set(theme_gray(base_size = 16))
  if(Sys.info()[["sysname"]] == "Darwin") { # MacOSか確認
    ## if(length(grep("BIZUDPGothic", systemfonts::system_fonts()[["name"]]))>0) 
    theme_update(text = element_text(family = "BIZUDGothic-Regular"))
    ##    else
    ## theme_update(text = element_text(family = "HiraMaruProN-W4"))
  }
  library(see)
  options(ggplot2.discrete.colour = function() scale_colour_material(),
          ggplot2.discrete.fill = function() scale_fill_material())
#+end_src

* COMMENT 講義概要
:PROPERTIES:
:ID:       6411F06A-9DF9-4E9A-AE03-52D6A80FE3EF
:END:
# 東京大学
- 第1回 : クラスタ分析の考え方と階層的方法
- *第2回 : 非階層的方法と分析の評価*

#+begin_src R :exports none :tangle no
  setwd("~/Desktop/lectures/u-tokyo/autumn/course")
  library(gt)
#+end_src
#+begin_src R :exports none 
  ### 第11講 サンプルコード
  library(conflicted)
  conflicts_prefer(
    dplyr::filter(),
    dplyr::select(),
    dplyr::lag(),
  )
  library(tidyverse)
  library(ggfortify)
  library(cluster)
#+end_src


  
* クラスタ分析の復習
** クラスタ分析
- クラスタ分析 (*cluster analysis*) の目的
  #+begin_quote
  個体の間に隠れている
  *集まり=クラスタ*
  を個体間の"距離"にもとづいて発見する方法
  #+end_quote
- 個体間の類似度・距離(非類似度)を定義
  - 同じクラスタに属する個体どうしは似通った性質
  - 異なるクラスタに属する個体どうしは異なる性質
- さらなるデータ解析やデータの可視化に利用
- 教師なし学習の代表的な手法の一つ

** クラスタ分析の考え方
- 階層的方法
  - データ点およびクラスタの間に *距離* を定義
  - 距離に基づいてグループ化
    - 近いものから順にクラスタを *凝集*
    - 近いものが同じクラスタに残るように *分割*
- 非階層的方法
  - クラスタの数を事前に指定
  - クラスタの *集まりの良さ* を評価する損失関数を定義
  - 損失関数を最小化するようにクラスタを形成

** COMMENT 階層的方法における凝集的手続き
1. データ・クラスタ間の距離を定義する
   - データ点とデータ点の距離
   - クラスタとクラスタの距離
2. データ点およびクラスタ間の距離を求める
3. 最も近い2つを統合し新たなクラスタを形成する
4. クラスタ数が1つになるまで2-3の手続きを繰り返す

** COMMENT 階層的方法における凝集的手続き
#+begin_leftcol60
1. データ・クラスタ間の距離を定義
   - データ点間の距離
   - クラスタ間の距離
2. データ点およびクラスタ間の距離を計算
3. 最も近い2つを統合し新たなクラスタを形成
4. クラスタ数が1つになるまで2-3の手続きを繰り返す
#+end_leftcol60    
#+begin_rightcol40
#+header: :width 600 :height 1000 :res 100
#+begin_src R :file figs/11_hclst.png :exports results :results graphics
  library(cluster)
  js_data <- read.csv("data/japan_social.csv", row.names=1)
  pref <- read.csv(file="data/prefecture.csv", row.names=1)
  rownames(js_data) <- pref$jp
  myPlot <- function(k) {
    tmpa <- js_data[8:14,]
    tmpb <- list(c(1,2,3,4,1,6,7),
		 c(1,2,3,1,1,6,7),
		 c(1,2,2,1,1,6,7),
		 c(1,2,2,1,1,6,1),
		 c(1,1,1,1,1,6,1),
		 c(1,1,1,1,1,1,1))
    clusplot(x=tmpa,
	     clus=c(1,2,3,4,5,6,7),
	     diss=FALSE,
	     stand=TRUE, lines=0, labels=3, 
	     main=NULL, sub=NULL, cex=1,
	     xlab="", ylab="", axes=FALSE,
	     xlim=c(-2.5,1.5), ylim=c(-1.5,2.2),
	     col.p="blue", col.clus="white", shade=FALSE)
    box()
    if(k>0) {
      for(i in 1:k) {
	clusplot(x=tmpa,
		 clus=tmpb[[i]],
		 diss=FALSE,
		 stand=TRUE, add=TRUE, span=FALSE,
		 lines=0, lwd=2, col.p="blue", col.clus="orange")
      }
    }
  }
  if(Sys.info()["sysname"]=="Darwin"){par(family="BIZUDGothic-Regular")}
  par(mfrow=c(4,2), mar=rep(0.5,4))
  myPlot(0)
  myPlot(1)
  myPlot(2)
  myPlot(3)
  myPlot(4)
  myPlot(5)
  myPlot(6)
  plot(agnes(scale(js_data[8:14,])), which.plots=2,
       main="",sub="",xlab="")
#+end_src

#+CAPTION: 凝集的手続きの例
#+NAME: fig:11_hclst
#+ATTR_HTML: height 100%
#+ATTR_LATEX: :width 0.6\linewidth
[[file:figs/11_hclst.png]]
#+end_rightcol40    

** 階層的クラスタリング
:PROPERTIES:
:ID:       41C635C9-2F63-41F1-B272-09A6D04A78B5
:END:
#+begin_leftcol
- 凝集的手続き
  1. データ・クラスタ間の距離を定義
     - データ点間の距離
     - クラスタ間の距離
  2. データ点およびクラスタ間の距離を計算
  3. 最も近い2つを統合し新たなクラスタを形成
  4. クラスタ数が1つになるまで2-3の手続きを繰り返す
#+end_leftcol
#+begin_rightcol
#+header: :width 600 :height 1000
#+begin_src R :file figs/11_hclst.png :exports results :results graphics
  library(cluster)
  js_data <- bind_cols(
    read_csv(file="data/japan_social.csv"),
    read_csv(file="data/prefecture.csv"))
  myPlot <- function(k) {
    if(Sys.info()["sysname"]=="Darwin"){par(family="BIZUDGothic-Regular")}
    tmpa <- js_data |>
      slice(8:14) |>
      select(2:6,jp) |>
      column_to_rownames(var = "jp")
    # tmpa <- js_data[8:14,]
    tmpb <- list(c(1,2,3,4,1,6,7),
                 c(1,2,3,1,1,6,7),
                 c(1,2,2,1,1,6,7),
                 c(1,2,2,1,1,6,1),
                 c(1,1,1,1,1,6,1),
                 c(1,1,1,1,1,1,1))
    clusplot(x=tmpa,
             clus=c(1,2,3,4,5,6,7),
             diss=FALSE,
             stand=TRUE, lines=0, labels=3, 
             main=NULL, sub=NULL, cex=1,
             xlim=c(-2.5,2.5), ylim=c(-2.5,2.5),
             xaxt="n", yaxt="n", ann=FALSE,
             col.p="blue", col.txt="darkgray", col.clus="white", shade=FALSE)
    if(k>0) {
      for(i in 1:k) {
        clusplot(x=tmpa,
                 clus=tmpb[[i]],
                 diss=FALSE, cex=0.2,
                 stand=TRUE, add=TRUE, span=FALSE,
                 lines=0, lwd=2, col.p="blue", col.clus="orange")
      }
    }
  }
  if(Sys.info()["sysname"]=="Darwin"){par(family="BIZUDGothic-Regular")}
  par(mfrow=c(4,2), mar=rep(0.5,4))
  for(i in 0:6) myPlot(i)
  js_data |>
    slice(8:14) |>
    select(2:6,jp) |>
    column_to_rownames(var = "jp") |>
    scale() |>
    agnes() |>
    plot(which.plots=2, main="",sub="",xlab="")
#+end_src

#+caption: 凝集的手続きの例
#+name: fig:11_hclst
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_hclst.png]]
#+end_rightcol


* 非階層的方法
** 非階層的方法の手続き
- 対象の変数 : \(\boldsymbol{X}=(X_1,X_2,\dotsc,X_{d})^{\mathsf{T}}\) (\(d\)次元)
- 観測データ : \(n\) 個の個体の組
  #+begin_quote
  \begin{equation}
    \{\boldsymbol{x}_{i}\}_{i=1}^{n}
    =
    \{(x_{i1},x_{i2},\dotsc,x_{id})^{\mathsf{T}}\}_{i=1}^{n}
  \end{equation}
  #+end_quote
- 個体とクラスタの対応 \(C\) を推定 
  #+begin_quote
  \begin{equation}
    C(i)
    =\text{(個体 \(i\) が属するクラスタ番号)}
  \end{equation}
  #+end_quote
  - 対応 \(C\) の *全体の良さ* を評価する損失関数を設定
  - 観測データ
    \(\{\boldsymbol{x}_{i}\}_{i=1}^{n}\)
    に最適な対応
    \(\{C(i)\}_{i=1}^{n}\) を決定

** @@latex:@@\(k\)-平均法の損失関数
- クラスタの個数 \(k\) を指定
- 2つの個体 \(i,i'\) の *近さ=損失* を距離の二乗で評価
  #+begin_quote
  \begin{equation}
    \|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2
    =
    \sum_{j=1}^{d}(x_{ij}-x_{i'j})^2
  \end{equation}
  #+end_quote
- 損失関数 \(W(C)\) : クラスタ内の平均の近さを評価
  #+begin_quote
  \begin{equation}
    W(C)
    =
    \sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}\|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2
  \end{equation}
  #+end_quote
  # (\(n_l\) はクラスタ \(l\) に属する個体の総数)

** @@latex:@@\(k\)-平均法の性質
- クラスタ \(l\) に属する個体の平均
  #+begin_quote
  \begin{equation}
    \bar{\boldsymbol{x}}_l
    =
    \frac{1}{n_l}\sum_{i:C(i)=l}\boldsymbol{x}_i
  \end{equation}
  #+end_quote
- 損失関数 \(W(C)\) の等価な表現
  #+begin_quote
  \begin{equation}
    W(C)
    =
    2\sum_{l=1}^k\sum_{i:C(i)=l}\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2
  \end{equation}
  #+end_quote
- 最適な対応 \(C\) : クラスタ内変動の総和が最小


* 演習
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 早稲田大学
** 問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 以下の問に答えなさい
  - 損失関数 \(W(C)\) の等価な表現を示しなさい
    #+begin_quote
    \begin{align}
      W(C)
      &=
        \sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}\|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2\\
      &=
        2\sum_{l=1}^k\sum_{i:C(i)=l}\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2
    \end{align}
    #+end_quote
  - 以下の\(\hat{\boldsymbol{\mu}}\)を求めなさい
    #+begin_quote
    \begin{equation}
      \hat{\boldsymbol{\mu}}
      =\arg\min_{\mu}
      \sum_{i:C(i)=l}\|\boldsymbol{x}_i-\boldsymbol{\mu}\|^2
    \end{equation}
    #+end_quote

** 解答例
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 対称性に注意して標本平均のまわりで展開
  #+begin_quote
  \begin{align}
    &\sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
      \|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2\\
    &=
      \sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
      \|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}+\bar{\boldsymbol{x}}_{l}-\boldsymbol{x}_{i'}\|^2\\
    &=
      \sum_{l=1}^k\frac{2}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
      \|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2\\
    &\quad-
      \sum_{l=1}^k\frac{2}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}
      (\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l})^{\mathsf{T}}
      (\boldsymbol{x}_{i'}-\bar{\boldsymbol{x}}_{l})
  \end{align}
  #+end_quote

#+reveal: split
- 中心化したデータの標本平均が0であることを利用
  #+begin_quote
  \begin{align}
    &=
      2\sum_{l=1}^k\sum_{i:C(i)=l}
      \|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2\\
    &\quad-
      \sum_{l=1}^k\frac{2}{n_l}
      \sum_{i:C(i)=l}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l})^{\mathsf{T}}
      \sum_{i':C(i')=l}(\boldsymbol{x}_{i'}-\bar{\boldsymbol{x}}_{l})\\
    &=
      2\sum_{l=1}^k\sum_{i:C(i)=l}
      \|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2
  \end{align}
  #+end_quote

#+reveal: split
- 以下の不等式が成立
  #+begin_quote
  \begin{align}
    \sum_{i:C(i)=l}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}\|^{2}
    &=
      \sum_{i:C(i)=l}
      \|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}
      +
      \bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu}\|^{2}\\
    &=
      \sum_{i:C(i)=l}
      \|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}\|^{2}
      +
      \sum_{i:C(i)=l}
      \|\bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu}\|^{2}\\
    &\quad+
      2\sum_{i:C(i)=l}
      (\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l})^{\mathsf{T}}
      (\bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu})\\
    &=
      \sum_{i:C(i)=l}
      \|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}\|^{2}
      +
      n_{l}\|\bar{\boldsymbol{x}}_{l}-\boldsymbol{\mu}\|^{2}\\
    &\ge
      \sum_{i:C(i)=l}
      \|\boldsymbol{x}_{i}-\bar{\boldsymbol{x}}_{l}\|^{2}
  \end{align}
  #+end_quote

#+reveal: split
- 等号の成立の条件より
  #+begin_quote
  \begin{equation}
    \hat{\boldsymbol{\mu}}
    =\arg\min_{\mu}
    \sum_{i:C(i)=l}\|\boldsymbol{x}_i-\boldsymbol{\mu}\|^2
    =\bar{\boldsymbol{x}}_{l}
  \end{equation}
  クラスタの標本平均を中心とすればよい
  #+end_quote


* 近似的な最適化
** クラスタ対応の最適化
- 最適化 : 損失関数 \(W(C)\) を最小とする \(C\) を決定
- 貪欲な \(C\) の探索
  - 原理的には全ての値を計算すればよい
  - 可能な \(C\) の数 : \(k^n\) 通り (有限個のパターン)
  - サンプル数 \(n\) が小さくない限り実時間での実行は不可能
- 近似的な \(C\) の探索
  - いくつかのアルゴリズムが提案されている
  - 基本的な考え方 : *Lloyd-Forgyのアルゴリズム*
    #+begin_quote
    標本平均と変動の平方和の性質を利用
    \begin{equation}
      \bar{\boldsymbol{x}}_l
      =\arg\min_{\mu}
      \sum_{i:C(i)=l}\|\boldsymbol{x}_i-\boldsymbol{\mu}\|^2
      \quad
      \text{(クラスタ\(l\)の標本平均)}
    \end{equation}
    #+end_quote

** Lloyd-Forgyのアルゴリズム
1. クラスタ中心の初期値 
   \(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_k\) を与える
2. 各データの所属クラスタ番号 \(C(i)\) を求める
   #+begin_quote
   \begin{equation}
     C(i)
     =
     \arg\min_l\|\boldsymbol{x}_i-\boldsymbol{\mu}_l\|
   \end{equation}
   #+end_quote
3. 各クラスタ中心 \(\boldsymbol{\mu}_l\;(l=1,2,\dotsc,k)\) を更新する
   #+begin_quote
   \begin{equation}
     \boldsymbol{\mu}_l
     =
     \frac{1}{n_l}\sum_{i:C(i)=l}\boldsymbol{x}_i,
     \quad
     n_l=|\{\boldsymbol{x}_i|C(i)=l\}|
   \end{equation}
   # (\(n_l\) は \(C(i)=l\) となるデータの総数)
   #+end_quote
4. 中心が変化しなくなるまで 2,3 を繰り返す

** アルゴリズムの性質
- 結果は *確率的*
  - 初期値 \(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_k\) に依存
  - アルゴリズムの成否は確率的なため，最適解が得られない場合もある
- 一般には複数の初期値をランダムに試して損失を最小とする解を採用する
- 平均の代わりにメドイド (medoid; 中心にある観測値) を用いる方法もある
  #+begin_quote
  \begin{equation}
    \boldsymbol{x}^{\mathrm{medoid}}_{l}
    =\arg\min_{\boldsymbol{x}_{i}}
    \sum_{i':C(i')=l}
    \|\boldsymbol{x}_{i}-\boldsymbol{x}_{i'}\|^2
  \end{equation}
  #+end_quote

** 事例
:PROPERTIES:
:ID:       5DF1EC41-AF81-47B0-A847-0EB32710158C
:END:
- 都道府県別好きなおむすびの具(一部)での例

#+begin_src R :file figs/11_nhclst0.png :exports results :results graphics
  om_data <- bind_cols(
    read_csv(file="data/omusubi.csv"),
    read_csv(file="data/prefecture.csv"))
  om_subset <- om_data |>
    select(ume:etc,jp) |>
    slice(-c(8:14,24:30)) |>
    column_to_rownames(var = "jp") |>
    set_names(c("梅","鮭","昆布","鰹","明太子","鱈子","ツナ","その他")) 
  n <- nrow(om_subset)

  jdx <- 1:ncol(om_subset) # jdx <- sample(ncol(om_subset),2)
  om_pca <- prcomp(om_subset[,jdx])

  if(Sys.info()["sysname"]=="Darwin"){par(family="BIZUDGothic-Regular")}
  plot(predict(om_pca),
       col="blue", pch=19, cex=0.5,
       xaxt="n", yaxt="n", ann=FALSE)
  text(transform(predict(om_pca), PC2=PC2+0.5),
       labels=rownames(om_subset), col="blue", cex=1)
#+end_src
#+caption: 非階層的クラスタリング
#+name: fig:11_nhclst0
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst0.png]]
#+reveal: split
#+begin_src R :file figs/11_nhclst1.png :exports results :results graphics
  ## 図示のための関数
  myPlot <- function(cntr) {
    dsy <- daisy(rbind(om_subset[,jdx],cntr))
    clst <- apply(as.matrix(dsy)[1:n,(1:k)+n],
                  1,
                  function(x){which.min(x)})
    plot(predict(om_pca), col=clst, pch=clst, type="n",
         xaxt="n", yaxt="n", ann=FALSE)
    text(predict(om_pca), labels=clst, col=clst, cex=1.5)
    points(predict(om_pca,newdata=cntr), col=1:k, pch=19, cex=2)
    cntr <- aggregate(. ~ clst,
                      data=data.frame(om_subset[,jdx],clst),
                      mean)[,-1]
    return(cntr)
  }
  set.seed(1212)
  k <- 5
  idx <- sample(n,k)
  cntr <- om_subset[idx,jdx]
  ## 
  cntr <- myPlot(cntr)
#+end_src
#+caption: Lloyd-Forgyのアルゴリズム (その1)
#+name: fig:11_nhclst1
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst1.png]]

#+reveal: split
#+begin_src R :file figs/11_nhclst2.png :exports results :results graphics
  cntr <- myPlot(cntr)
#+end_src
#+caption: Lloyd-Forgyのアルゴリズム (その2)
#+name: fig:11_nhclst2
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst2.png]]

#+reveal: split
#+begin_src R :file figs/11_nhclst3.png :exports results :results graphics
  cntr <- myPlot(cntr)
#+end_src
#+caption: Lloyd-Forgyのアルゴリズム (その3)
#+name: fig:11_nhclst3
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst3.png]]

#+reveal: split
#+begin_src R :file figs/11_nhclst4.png :exports results :results graphics
   cntr <- myPlot(cntr)
#+end_src
#+caption: Lloyd-Forgyのアルゴリズム (その4)
#+name: fig:11_nhclst4
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst4.png]]

#+reveal: split
#+begin_src R :file figs/11_nhclst5.png :exports results :results graphics
   cntr <- myPlot(cntr)
#+end_src
#+caption: Lloyd-Forgyのアルゴリズム (その5)
#+name: fig:11_nhclst5
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst5.png]]

#+reveal: split
#+begin_src R :file figs/11_nhclst6.png :exports results :results graphics
   cntr <- myPlot(cntr)
#+end_src
#+caption: Lloyd-Forgyのアルゴリズム (その6)
#+name: fig:11_nhclst6
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst6.png]]

#+reveal: split
#+begin_src R :file figs/11_nhclst.png :exports results :results graphics
   if(Sys.info()["sysname"]=="Darwin"){par(family="BIZUDGothic-Regular")}
   dsy <- daisy(rbind(om_subset[,jdx],cntr))
   clst <- apply(as.matrix(dsy)[1:n,(1:k)+n],
                 1,
                 function(x){which.min(x)})
   plot(predict(om_pca),
        col="blue", pch=19, cex=0.5,
        xaxt="n", yaxt="n", ann=FALSE)
   text(transform(predict(om_pca), PC2=PC2+0.5),
        labels=rownames(om_subset), col=clst, cex=1)
#+end_src
#+caption: クラスタリングの結果
#+name: fig:11_nhclst
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_nhclst.png]]
 

* COMMENT 実習
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 東京大学
** R : 関数 ~kmeans()~
:PROPERTIES:
:ID:       BF14B712-D1D7-4CDA-BE9C-3BDCFDD7822F
:END:
- \(k\)-平均法を実行するための標準的な関数
  #+begin_src R :eval no
    kmeans(x, centers, iter.max = 10, nstart = 1,
	   algorithm = "Hartigan-Wong")
    ## x: データフレーム
    ## centers: クラスタ数
    ## iter.max: 最大繰り返し数
    ## nstart: 初期値の候補数
    ## algorithm: 最適化法の指定．他に "Lloyd", "Forgy", "MacQueen" が指定可
  #+end_src
- 結果は変数のスケールにも依存
  - 例えば測定値の単位により異なる
  - 必要ならば主成分分析の場合と同様にデータの標準化を行う

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:ID:       F4543112-3BC2-4689-8887-5510E8A66EE1
:END:
- データセット ~japan_social.csv~ を用いて
  以下を確認しなさい
  #+begin_src R :eval no
    js_data <- read.csv("data/japan_social.csv", row.names=1)
  #+end_src
  - 関数 ~kmeans()~ を用いて
    各変数平均0，分散1に標準化
    (関数 ~scale()~ を利用)
    したデータを7クラスタに分割しなさい
  - 各クラスタ内の県名を表示しなさい
  - 関数 ~clusplot()~ を用いて
    2次元散布図に各クラスタを表示しなさい
    (~?clusplot~ 参照)

#+begin_src R :eval no :exports none :tangle yes
  ### 
  ### 練習問題 関数 kmeans による非階層的クラスタリング
  ### 
  
  ## データの読み込み
  js_data <- read.csv(file="data/japan_social.csv", row.names=1)
  
  ## k-平均法の実行: 
  set.seed(1234) # 必要に応じて初期値の乱数のシード
  k <- 7 # 分割数を指定
  js_km <- kmeans(scale(js_data), # 標準化
		  centers=k, # クラスタ数
		  nstart=20) # 初期値を20回変更して試す
  
  ## 各クラスター内の県名を表示
  for(i in 1:k){
    cat("=== cluster",i,"===\n")
    print(names(which(js_km$cluster==i)))
  }
  
  ## clusplotによるクラスタの表示
  library(cluster) # clusplot を利用するため
  clusplot(x=js_data,
	   clus=js_km$cluster, # クラスタ番号
	   stand=TRUE, # 標準化したデータで表示
	   lines=0, labels=3, # 表示の指定
	   main=NULL, sub=NULL, cex=0.8, # タイトルなどの調整
	   col.p=rainbow(k)[js_km$cluster], # 虹色で色付け
	   col.clus="orange", shade=FALSE)	 # クラスタ囲みの指定
#+end_src

** R : 関数 ~cluster::pam()~
:PROPERTIES:
:ID:       23F6D185-357C-4973-95B4-184D0335F0BC
:END:
- \(k\)-メドイド法を実行するための関数
  #+begin_src R :eval no
    pam(x, k, metric = "euclidean", stand = FALSE)
    ## x: データフレーム，または距離行列 
    ## k: クラスタの数
    ## metric: 距離の指定(xがデータフレームの場合)．他に "manhattan" が指定可
    ## stand: 標準化(平均0，絶対偏差1)
  #+end_src
  - 詳細は ~?cluster::pam~ を参照

** COMMENT 演習: 非階層的クラスタリング
:PROPERTIES:
:reveal_background: #EEEEFF
:END:
- [[./code/12-kmeans.r][12-kmeans.r]] を確認してみよう

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:ID:       95C3E2F9-62C2-48DA-A6D2-F3315F356388
:END:
- データセット ~japan_social.csv~ を用いて
  以下を確認しなさい
  - 関数 ~pam()~ を用いて
    各変数平均0，絶対偏差1に標準化したデータを7クラスタに分割しなさい
  - 各クラスタ内の県名を表示しなさい
  - 関数 ~clusplot()~ を用いて
    2次元散布図に各クラスタを表示しなさい
- 余裕がある人はデータセット ~omusubi.csv~ でも確認しなさい

#+begin_src R :eval no :exports none :tangle yes
  ### 
  ### 練習問題 関数 pam による非階層的クラスタリング
  ### 

  ## k-medoids の実行
  js_pam <- pam(js_data,
                stand=TRUE,
                k=k)

  ## 各クラスター内の県名を表示
  for(i in 1:k){
      cat("=== cluster",i,"===\n")
      print(names(which(js_pam$clustering==i)))
  }

  ## クラスタの表示
  clusplot(x=js_data,
           clus=js_pam$clustering,
           stand=TRUE,
           lines=0, labels=3, 
           main=NULL, sub=NULL, cex=0.8,
           col.p=rainbow(k)[js_pam$clustering],
           col.clus="orange", shade=FALSE)

  ## 
  om_data <- read.csv(file="data/omusubi.csv", row.names=1)

  ## k-medoids の実行
  k <- 6
  om_pam <- pam(daisy(sqrt(om_data)), # Hellinger距離 (スケールをさぼっている)
                k=k)

  ## 各クラスター内の県名を表示
  for(i in 1:k){
      cat("=== cluster",i,"===\n")
      print(names(which(om_pam$clustering==i)))
  }

  ## クラスタの表示
  clusplot(x=om_data,
           clus=om_pam$clustering,
           stand=TRUE,
           lines=0, labels=3, 
           main=NULL, sub=NULL, cex=0.8,
           col.p=rainbow(k)[om_pam$clustering],
           col.clus="orange", shade=FALSE)
#+end_src


* 解析事例
# 早稲田大学
** 都道府県別の社会生活統計指標
:PROPERTIES:
:ID:       4724A63D-8650-4583-8996-DD8E06C7BA77
:END:
- データの属性
  #+begin_example
  Forest : 森林面積割合 (%) 2014年
  Agri : 就業者１人当たり農業産出額(販売農家）(万円) 2014年
  Ratio : 全国総人口に占める人口割合 (%) 2015年
  Land : 土地生産性（耕地面積１ヘクタール当たり）(万円) 2014年
  Goods : 商業年間商品販売額［卸売業＋小売業］（事業所当たり）(百万円) 2013年
  #+end_example
- 平均0，分散1に正規化して解析

#+reveal: split
- ユークリッド距離 + k-平均法 
  #+begin_src R :exports none
    ### 総務省統計局の統計データによる例
    library(cluster)
    library(RColorBrewer)
    js_data <- bind_cols(
      read_csv(file="data/japan_social.csv"),
      read_csv(file="data/prefecture.csv"))

    ## k-平均法の実行: 
    set.seed(1234)
    k <- 7 # 分割数を指定
    js_km <- js_data |>
      select(Pref,Forest:Goods) |>
      column_to_rownames(var = "Pref") |>
      scale() |>
      kmeans(centers=k, nstart=20)
  #+end_src
  #+begin_src R :exports results
    ## 結果の確認 (各クラスター内の県名を表示)
    for(i in 1:k){
           cat("=== cluster",i,"===\n")
           print(js_data[["jp"]][js_km$cluster==i])
    }
  #+end_src
  
#+reveal: split
#+begin_src R :file figs/11_jskmeans.png :exports results :results graphics
  js_km |>
    autoplot(data = js_data,
             frame = TRUE,
             frame.type = "convex",
             label = TRUE,
             label.repel = TRUE,
             label.show.legend = FALSE)
#+end_src
#+caption: ユークリッド距離 + k-平均法 
#+name: fig:11_jskmeans
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_jskmeans.png]]

#+reveal: split
- ユークリッド距離 + k-メドイド法 
  #+begin_src R :exports results :tangle yes
    ## k-medoids の実行
    js_pam <- js_data |>
      select(Pref,Forest:Goods) |>
      column_to_rownames(var = "Pref") |>
      scale() |>
      pam(k=k)
    ## metric="manhattan", stand=TRUE)
  #+end_src
  #+begin_src R :exports results
    ## 結果の確認 (各クラスター内の県名を表示)
    for(i in 1:k){
           cat("=== cluster",i,"===\n")
           print(js_data[["jp"]][js_pam$clustering==i])
    }
  #+end_src

#+reveal: split
#+begin_src R :file figs/11_jspam.png :exports results :results graphics
  js_pam |>
    autoplot(# data = js_data,
             frame = TRUE,
             frame.type = "convex",
             label = TRUE,
             label.repel = TRUE,
             label.show.legend = FALSE)
#+end_src
#+caption: ユークリッド距離 + k-メドイド法 
#+name: fig:11_jspam
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_jspam.png]]

** 都道府県別好きなおむすびの具
:PROPERTIES:
:ID:       0EB616B0-E044-4E30-BB4C-602C3A1C135F
:END:
- データの属性
  #+begin_example
  Q2. おむすびの具では何が一番好きですか？
     A.梅 B.鮭 C.昆布 D.かつお E.明太子 F.たらこ Ｇ.ツナ H.その他
  【回答者数】
   男性	9,702人	    32.0%
   女性    20,616人	    68.0%
   総数    30,318人	   100.0%
  #+end_example
  - 回答を県別に集計
- Hellinger距離を利用
  #+begin_quote
  \(\boldsymbol{p},\boldsymbol{q}\)
  を確率ベクトルとして
  定義される確率分布の距離
  \begin{equation}
    d_{\mathrm{hel}}(\boldsymbol{p},\boldsymbol{q})
    =
    \frac{1}{\sqrt{2}}d_{\mathrm{euc}}(\sqrt{\boldsymbol{p}},\sqrt{\boldsymbol{q}})
  \end{equation}
  #+end_quote

#+reveal: split
- Hellinger距離 + k-メドイド法
  #+begin_src R :exports none
    #' データの読み込み("omusubi.csv"を用いる)
    om_data <- bind_cols(
      read_csv(file="data/omusubi.csv"),
      read_csv(file="data/prefecture.csv"))
    #' k-medoids の実行
    k <- 6
    om_pam <- om_data |>
      select(ume:etc,jp) |>
      column_to_rownames(var = "jp") |>
      sqrt() |> # Hellinger dist. daisy() |> 
      pam(k = k) # stand=TRUE は不要
  #+end_src
  #+begin_src R :exports results
    ## 結果の確認 (各クラスター内の県名を表示)
    for(i in 1:k){
      cat("=== cluster",i,"===\n")
      print(names(which(om_pam$clustering==i)))
    }
  #+end_src

#+reveal: split
#+begin_src R :file figs/11_ompam.png :exports results :results graphics
  om_pam |>
    autoplot(# data = om_data,
             frame = TRUE,
             frame.type = "convex",
             label = TRUE,
             label.repel = TRUE,
             label.family = "BIZUDGothic-Regular",
             label.show.legend = FALSE)
#+end_src

#+caption: Hellinger距離 + k-メドイド法 
#+name: fig:11_ompam
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_ompam.png]]
   

* クラスタ構造の評価
** 階層的方法の評価
- 評価の対象
  - データ \(\boldsymbol{x}_i\) と最初に統合されたクラスタ \(C\) の距離
    #+begin_quote
    \begin{equation}
      d_i
      =
      D({\boldsymbol{x}_i},C)
    \end{equation}
    #+end_quote
  - 最後に統合された2つのクラスタ \(C',C''\) の距離
    #+begin_quote
    \begin{equation}
      D
      =
      D(C',C'')
    \end{equation}
    #+end_quote
- *凝集係数* (agglomerative coefficient)
  #+begin_quote
  \begin{equation}
    AC
    =
    \frac{1}{n}\sum_{i=1}^{n}\left(1-\frac{d_i}{D}\right)
  \end{equation}
  #+end_quote

** 凝集係数の性質
- 定義より
  #+begin_quote
  \begin{equation}
    0\le AC\le 1
  \end{equation}
  #+end_quote
  - 1に近いほどクラスタ構造が明瞭
- banner plot: 各 \((1-{d_i}/{D})\) を並べた棒グラフ
- banner plot の面積比として視覚化 

** 非階層的方法の評価
- 評価の対象
  - \(\boldsymbol{x}_i\) を含むクラスタ \(C^1\) と \(\boldsymbol{x}_i\) の距離
    #+begin_quote
    \begin{equation}
      d^1_i=D({\boldsymbol{x}_i},C^1\setminus{\boldsymbol{x}_i})
    \end{equation}
    #+end_quote
  - 一番近いクラスタ \(C^2\) と \(\boldsymbol{x}_i\) の距離
    #+begin_quote
    \begin{equation}
      d^2_i=D({\boldsymbol{x}_i},C^2)
    \end{equation}
    #+end_quote
- *シルエット係数* (silhouette coefficient)
  #+begin_quote
  \begin{equation}
    S_i
    =
    \frac{d^2_i-d^1_i}{\max(d^1_i,d^2_i)}
  \end{equation}
  #+end_quote

  # #   - データ \(\boldsymbol{x}_i\) が含まれているクラスタ: \(C^1\)
  # #   - \(C^1\) 以外で \(\boldsymbol{x}_i\) に一番近いクラスタ: \(C^2\)
  # # - \(\boldsymbol{x}_i\) を除いたクラスタ \(C^1\) とデータ \(\boldsymbol{x}_i\) の距離:
  # #   # #+begin_export latex
  # #   \begin{equation}
  # #	 d^1_i
  # #	 =
  # #	 D({\boldsymbol{x}_i},C^1\setminus{\boldsymbol{x}_i})
  # #   \end{equation}
  # #   # #+end_export
  # # - \(C^2\) と \(\boldsymbol{x}_i\) の距離:
  # #   # #+begin_export latex
  # #   \begin{equation}
  # #	 d^2_i
  # #	 =
  # #	 D({\boldsymbol{x}_i},C^2)
  # #   \end{equation}
  # #   # #+end_export

** シルエット係数の性質
- 定義より
  #+begin_quote
  \begin{equation}
    -1\le S_i\le 1
  \end{equation}
  #+end_quote
  - 1に近いほど適切なクラスタリング
- 全体の良さを評価するには \(S_i\) の平均を用いる
- 距離の計算を適切に行えば階層的方法でも利用可


* 演習
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 早稲田大学
** 問題
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 以下の問に答えなさい
  - 群平均法において凝集係数が以下を満たすことを示しなさい
    #+begin_quote
    \begin{equation}
      0\le AC\le 1
    \end{equation}
    #+end_quote
  - シルエット係数が以下を満たすことを示しなさい
    #+begin_quote
    \begin{equation}
      -1\le S_i\le 1
    \end{equation}
    #+end_quote

** 解答例
:PROPERTIES:
:reveal_background: #fef4f4
:END:
- 2つのクラスタ\(C_{a},C_{b}\)が最も近いとする
  #+begin_quote
  \begin{equation}
       D(C_{c},C_{d})\ge D(C_{a},C_{b}),
       \quad\forall c,d
  \end{equation}
  #+end_quote

#+reveal: split
- 統合して計算される距離では下が成立
  #+begin_quote
  \begin{align}
       D(C_{a}+C_{b}, C_{c})
       &=
         \frac{|C_{a}|D(C_{a},C_{c})+|C_{b}|D(C_{b},C_{c})}{|C_{a}|+|C_{b}|}\\
       &\ge
         \frac{|C_{a}|D(C_{a},C_{b})+|C_{b}|D(C_{a},C_{b})}{|C_{a}|+|C_{b}|}\\
       &=D(C_{a},C_{b})
  \end{align}
  統合した結果，それより短い距離が現れることはない
  #+end_quote

#+reveal: split
- 以上より
  #+begin_quote
  \begin{equation}
       0\le d_{i}\le D
  \end{equation}
  \begin{equation}
       0\le 1-\frac{d_i}{D}\le 1
  \end{equation}
  よって
  \begin{equation}
       0\le AC\le 1
  \end{equation}
  #+end_quote

#+reveal: split
- 非負値の大小関係に注意する
  #+begin_quote
  \begin{equation}
       -\max(d^{1}_{i},d^{2}_{i})\le d^{2}_{i}-d^{1}_{i}\le\max(d^{1}_{i},d^{2}_{i})
  \end{equation}
  より
  \begin{equation}
       -1\le S_i\le 1
  \end{equation}
  #+end_quote
  

* COMMENT 実習
:PROPERTIES:
:reveal_background: #fef4f4
:END:
# 東京大学
** R : 関数 ~cluster::agnes()~ の場合
:PROPERTIES:
:ID:       5EFEBBAA-7795-4ED3-9A25-C5506764F4B9
:END:
- 関数 ~agnes()~ による凝集係数の取得
  #+begin_src R :eval no
    ### 凝集係数の取得
    summary(agnes(x, # データフレーム
                  stand = TRUE, # 正規化
		  metric = "euclidean", # ユークリッド距離
		  method = "average") # 群平均法
	    )$ac 
    ### 凝集係数の視覚化 (banner plot)
    plot(agnes(x), # 階層的クラスタリングの結果
         which.plots=1) # banner plot を選択
    ## plot および cluster::bannerplot のオプションを参考
  #+end_src

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:ID:       EAEFCCDA-22C8-40F1-8F9A-462C6E2AD824
:END:
- データセット ~japan_social.csv~ を用いて
  以下を検討しなさい
  - 関数 ~agnes()~ を用いて階層的クラスタリングを行いなさい
    - 標準化: 行う
    - データ距離: ユークリッド距離，およびマンハッタン距離
    - クラスタ距離: 群平均法
  - 凝集係数を用いて2つの距離の評価を行いなさい

#+begin_src R :eval no :exports none :tangle yes
  ###
  ### 練習問題 凝集係数による距離の検討
  ### 

  ## データの読み込み (既に読み込んでいれば不要)
  js_data <- read.csv("data/japan_social.csv", row.names=1)

  ## ユークリッド距離による階層的クラスタリング
  js_agn.euc <- agnes(js_data,
			metric="euclidean", # データ距離
			stand=TRUE,	    # 標準化
			method="average")   # クラスタ距離
  plot(js_agn.euc, which.plots=2, # デンドログラムの表示
	    main="euclidean") 

  ## マンハッタン距離による階層的クラスタリング
  js_agn.man <- agnes(js_data,
			metric="manhattan",
			stand=TRUE,
			method="average")
  plot(js_agn.man, which.plots=2,
	    main="manhattan")

  ## データ毎の凝集係数の表示
  plot(js_agn.euc, which.plots=1, # banner plotの表示
	    nmax.lab=50,   # 表示するラベルの上限 (標準は40)
	    max.strlen=5,  # 表示するラベルの文字数の上限
	    main="euclidean")

  plot(js_agn.man, which.plots=1,
	    nmax.lab=50,  
	    max.strlen=5,
	    main="manhattan")

  ## 一部のデータの距離が大きいと凝集係数は大きくなりがち (理由を考えてみよう)
  ## 北海道，東京，宮崎，鹿児島を除いて再計算する 
  summary(agnes(js_data[-c(1,13,45,46),],
		     metric="euclidean",
		     stand=TRUE,
		     method="average"))$ac
  summary(agnes(js_data[-c(1,13,45,46),],
		     metric="manhattan",
		     stand=TRUE,
		     method="average"))$ac
  ## ユークリッド距離の方が凝集係数は大きいことがわかる
  ## 個別の係数の確認
  plot(agnes(js_data[-c(1,13,45,46),],
		  metric="euclidean",
		  stand=TRUE,
		  method="average"),
	    which.plots=1,
	    nmax.lab=50,  
	    max.strlen=5,
	    main="euclidean")
  plot(agnes(js_data[-c(1,13,45,46),],
		  metric="manhattan",
		  stand=TRUE,
		  method="average"),
	    which.plots=1,
	    nmax.lab=50,  
	    max.strlen=5,
	    main="manhattan")
#+end_src

** R : 関数 ~cluster::pam()~ の場合
:PROPERTIES:
:ID:       6AF1B625-7D2F-40F4-9AC5-236684343A73
:END:
- 関数 ~pam()~ によるシルエット係数の取得
  #+begin_src R :eval no
    ### シルエット係数関連の情報取得
    summary(pam(x, k))$silinfo
    ### 各データのシルエット係数
    summary(pam(x, k))$silinfo$widths
    ### 各クラスタのシルエット係数の平均
    summary(pam(x, k))$silinfo$clus.avg.widths
    ### シルエット係数の平均
    summary(pam(om_dsy,k=k))$silinfo$avg.width
    ### シルエット係数の視覚化 (silhouette plot)
    plot(pam(om_dsy,k=k), which.plot=2)
    ## plot および cluster::silhouette のオプションを参考
  #+end_src

** COMMENT 演習: クラスタ分析の評価
:PROPERTIES:
:reveal_background: #EEEEFF
:END:
- [[./code/12-eval.r][12-eval.r]] を確認してみよう

** 練習問題
:PROPERTIES:
:reveal_background: #fef4f4
:ID:       AA8EF433-1EEC-4705-BFB1-EE300889D6F1
:END:
- データセット ~omusubi.csv~ を用いて
  以下を検討しなさい
  - Hellinger距離を用いて距離行列を計算しなさい
    #+begin_quote
    \(\boldsymbol{p},\boldsymbol{q}\)
    を確率ベクトルとして
    定義される確率分布の距離
    \begin{equation}
      d_{\mathrm{hel}}(\boldsymbol{p},\boldsymbol{q})
      =
      \frac{1}{\sqrt{2}}d_{\mathrm{euc}}(\sqrt{\boldsymbol{p}},\sqrt{\boldsymbol{q}})
    \end{equation}
    #+end_quote
  - クラスタ数4-10のシルエット係数を比較しなさい
  - 適当と思われるクラスタ数による分析を行いなさい

#+begin_src R :eval no :exports none :tangle yes
  ### 
  ### 練習問題 シルエット係数によるクラスタ数の検討
  ### 

  ## データの読み込み (既に読み込んでいれば不要)
  om_data <- read.csv(file="data/omusubi.csv", row.names=1)

  ## Hellinger距離の計算
  om_dsy <- 1/sqrt(2)*daisy(sqrt(om_data/100))

  ## クラスタ数 4-10 で平均シルエット係数を確認
  for(k in 4:10){
	   cat("k =",k," ")
	   print(summary(pam(om_dsy,k=k))$silinfo$avg.width)
  }

  ## 7-9 のシルエット係数を視覚化
  plot(pam(om_dsy,k=7), which.plot=2,
	    nmax.lab=50, # 表示するラベルの上限 (標準は40)
	    max.strlen=5, # 表示するラベルの文字数の上限
	    cex.names=0.1) # ラベルの文字の大きさの調整
  plot(pam(om_dsy,k=8), which.plot=2,
	    nmax.lab=50, max.strlen=5, cex.names=0.1)
  plot(pam(om_dsy,k=9), which.plot=2,
	    nmax.lab=50, max.strlen=5, cex.names=0.1)

  ## k=8が悪いシルエット係数が少ないという意味で良さそうなのでクラスタの結果を表示
  k <- 8
  om_pam <- pam(om_dsy,k=k)
  plot(om_pam, which.plot=1, 
	    stand=TRUE,
	    lines=0, labels=3, 
	    main="", sub=NULL, cex=0.8, # タイトルと文字の大きさの調整
	    col.p=rainbow(k)[om_pam$clustering], # クラスタ番号ごとに色付け
	    col.clus="orange", shade=FALSE) # クラスタを楕円で表示
  ## cluster::clusplot のオプションを参考
  ## クラスタリングの結果は om_pam$clustering に保管されている
#+end_src


* 解析事例
# 早稲田大学
** 都道府県別の社会生活統計指標
:PROPERTIES:
:ID:       491B5831-261E-423F-BFCF-E6B0721A90B1
:END:
- 凝集係数を用いて階層的方法の距離を検討
- ユークリッド距離とマンハッタン距離を比較
  - 正規化は共通 (平均0，絶対偏差1)
  - クラスタ距離は群平均法
    
#+reveal: split
#+begin_src R :file figs/11_jsbannereuc.png :exports results :results graphics
  js_agnes_euc <- js_data |>
    select(Forest:Goods,jp) |>
    column_to_rownames(var = "jp") |>
    agnes(metric = "euclidean",
          stand = TRUE,
          method = "average")
  if(Sys.info()["sysname"]=="Darwin"){par(family="BIZUDGothic-Regular")}
  plot(js_agnes_euc, which.plots=1,
       sub = paste("凝集係数 = ",round(js_agnes_euc$ac, digits = 3)),
       nmax.lab=50, 
       max.strlen=5, 
       cex.axis=0.5,
       main="")
#+end_src
#+caption: 凝集係数 (ユークリッド距離)
#+name: fig:11_jsbannereuc
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
file:figs/11_jsbannereuc.png
  
#+reveal: split
#+begin_src R :file figs/11_jsdendroeuc.png :exports results :results graphics
  js_agnes_euc |>
    as.dendrogram() |>
    ggdendrogram(rotate = TRUE, theme_dendro = FALSE) +
    labs(title = "Euclid 距離 + 群平均法",
         x = "県名", y = "距離") +
    theme(axis.text.y = element_text(size = 9))
#+end_src
#+caption: デンドログラム (ユークリッド距離)
#+name: fig:11_jsdendroeuc
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
file:figs/11_jsdendroeuc.png

#+reveal: split
#+begin_src R :file figs/11_jsbannerman.png :exports results :results graphics
  js_agnes_man <- js_data |>
    select(Forest:Goods,jp) |>
    column_to_rownames(var = "jp") |>
    agnes(metric = "manhattan",
          stand = TRUE,
          method = "average")
  if(Sys.info()["sysname"]=="Darwin"){par(family="BIZUDGothic-Regular")}
  plot(js_agnes_man, which.plots=1,
       sub = paste("凝集係数 = ",round(js_agnes_man$ac, digits = 3)),
       nmax.lab=50, 
       max.strlen=5, 
       cex.axis=0.5,
       main="")
#+end_src
#+caption: 凝集係数 (マンハッタン距離)
#+name: fig:11_jsbannerman
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
file:figs/11_jsbannerman.png
  
#+reveal: split
#+begin_src R :file figs/11_jsdendroman.png :exports results :results graphics
  js_agnes_man |>
    as.dendrogram() |>
    ggdendrogram(rotate = TRUE, theme_dendro = FALSE) +
    labs(title = "Manhattan 距離 + 群平均法",
         x = "県名", y = "距離") +
    theme(axis.text.y = element_text(size = 9))
#+end_src
#+caption: デンドログラム (マンハッタン距離)
#+name: fig:11_jsdendroman
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
file:figs/11_jsdendroman.png

#+reveal: split
- 一部のデータの距離が大きいと凝集係数は大きくなりがち (理由を考えてみよう)
- 北海道，東京，宮崎，鹿児島を除いて再計算する 
  #+begin_src R :exports results
    cat("凝集係数 (ユークリッド距離)\n")
    (js_data |>
      select(Forest:Goods,jp) |>
      slice(-c(1,13,45,46)) |>
      column_to_rownames(var = "jp") |>
      agnes(metric = "euclidean",
            stand = TRUE,
            method = "average"))$ac |> round(digits = 3)
    cat("凝集係数 (マンハッタン距離)\n")
    (js_data |>
      select(Forest:Goods,jp) |>
      slice(-c(1,13,45,46)) |>
      column_to_rownames(var = "jp") |>
      agnes(metric = "manhattan",
            stand = TRUE,
            method = "average"))$ac |> round(digits = 3)
  #+end_src
     
** 都道府県別好きなおむすびの具
:PROPERTIES:
:ID:       930CC123-5CB9-4266-B82D-D0095A28DBF9
:END:
- シルエット係数を用いて非階層的方法のクラスタ数を検討
  - データ距離はHellinger距離
  - クラスタ距離は群平均法
- クラスタ数を4-10として比較
  #+begin_src R :exports results :tangle yes
    ## さまざまなクラスタ数で平均シルエット係数を確認
    om_data_hel <- om_data |>
      select(ume:etc,jp) |>
      column_to_rownames(var = "jp") |>
      sqrt() # Hellinger dist.用
    cat("シルエット係数\n")
    for(k in 4:10){
      cat("k =",k," ")
      print(pam(om_data_hel, k = k)$silinfo$avg.width, digits = 3)
    }
  #+end_src

#+reveal: split
#+begin_src R :file figs/11_omsil7.png :exports results :results graphics
  om_data_hel |>
    pam(k = 7) |>
    silhouette() |>
    autoplot() +
    labs(x = "都道府県(クラスタ)",
         y = paste("シルエット係数")
#+end_src
#+caption: シルエット係数の分布 (k=7)
#+name: fig:11_omsil7
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
file:figs/11_omsil7.png

#+reveal: split
#+begin_src R :file figs/11_omsil8.png :exports results :results graphics
  om_data_hel |>
    pam(k = 8) |>
    silhouette() |>
    autoplot() +
    labs(x = "都道府県(クラスタ)",
         y = paste("シルエット係数")
#+end_src
#+caption: シルエット係数の分布 (k=8)
#+name: fig:11_omsil8
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
file:figs/11_omsil8.png

#+reveal: split
#+begin_src R :file figs/11_omsil9.png :exports results :results graphics
  om_data_hel |>
    pam(k = 9) |>
    silhouette() |>
    autoplot() +
    labs(x = "都道府県(クラスタ)",
         y = paste("シルエット係数")
#+end_src
#+caption: シルエット係数の分布 (k=9)
#+name: fig:11_omsil9
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
file:figs/11_omsil9.png

#+reveal: split
#+begin_src R :file figs/11_omclusplot8.png :exports results :results graphics
  ## k=8が良さそう
  om_data_hel |>
    pam(k = 8) |>
    autoplot(# data = om_data,
             frame = TRUE,
             frame.type = "convex",
             label = TRUE,
             label.repel = TRUE,
             label.family = "BIZUDGothic-Regular",
             label.show.legend = FALSE)
#+end_src
#+caption: 非階層的クラスタリング (k=8)
#+name: fig:11_omclusplot8
#+attr_html: height 100%
#+attr_latex: :width 0.6\linewidth
[[file:figs/11_omclusplot8.png]]


* 次回の予定
- *第1日 : 時系列の基本モデル*
- 第2日 : モデルの推定と予測  

* Footnotes
* COMMENT ローカル変数
# Local Variables:
# org-latex-listings: minted
# End:
